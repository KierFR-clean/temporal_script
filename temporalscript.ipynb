{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgm+u70E9EBCU7HI9GRy+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KierFR-clean/temporal_script/blob/main/temporalscript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries"
      ],
      "metadata": {
        "id": "a8pQPJstX8YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery pandas\n"
      ],
      "metadata": {
        "id": "ixuLtd1KYCNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "authenticate by service account key/gcloud CLI."
      ],
      "metadata": {
        "id": "1svFw_HKYHh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "fUfjZB67YTIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using a service account key file:"
      ],
      "metadata": {
        "id": "GfCASTRsYWe0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wuo5QDV5d1Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Uploadservice account\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ser_acc_path.json\"\n"
      ],
      "metadata": {
        "id": "mPEwLAqsYdVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "script adapted to run in colab/bigquery instead localfile"
      ],
      "metadata": {
        "id": "RJL-h5ELYm6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "\n",
        "#  debug log\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GoogleClusterDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor script\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, project_id: str = \"google.com:google-cluster-data\"):\n",
        "        \"\"\"\n",
        "        Init with proj id\n",
        "        \"\"\"\n",
        "        self.project_id = project_id\n",
        "        self.client = bigquery.Client()\n",
        "        self.clusters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
        "\n",
        "        # tables in the dataset\n",
        "        self.tables = [\n",
        "            'machine_events',\n",
        "            'machine_attributes',\n",
        "            'collection_events',\n",
        "            'instance_events',\n",
        "            'instance_usage'\n",
        "        ]\n",
        "\n",
        "    def convert_time_to_datetime(self, df: pd.DataFrame, time_col: str = 'time') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        timestamp -> microseconds\n",
        "        \"\"\"\n",
        "        if time_col in df.columns:\n",
        "            trace_start = datetime(2019, 5, 1, 7, 0, 0)  # Start timestamp\n",
        "            df[time_col] = pd.to_datetime(trace_start + pd.to_timedelta(df[time_col] - 600_000_000, unit='us'))\n",
        "        return df\n",
        "\n",
        "    def query_table(self, cluster: str, table: str, limit: int = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Query a table from dataframe\n",
        "        \"\"\"\n",
        "        dataset_id = f\"clusterdata_2019_{cluster}\"\n",
        "        table_id = f\"{self.project_id}.{dataset_id}.{table}\"\n",
        "        query = f\"SELECT * FROM `{table_id}`\"\n",
        "\n",
        "        if limit:\n",
        "            query += f\" LIMIT {limit}\"\n",
        "\n",
        "        logger.info(f\"Querying {table_id} with limit {limit}\")\n",
        "\n",
        "        try:\n",
        "            df = self.client.query(query).to_dataframe()\n",
        "            logger.info(f\"Retrieved {len(df)} rows from {table_id}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error querying {table_id}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def preprocess_instance_events(self, cluster: str, limit: int = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess instance events\n",
        "        \"\"\"\n",
        "        df = self.query_table(cluster, 'instance_events', limit)\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        # microscs ts -> datetime\n",
        "        df = self.convert_time_to_datetime(df)\n",
        "\n",
        "        # Parse rq & sc\n",
        "        if 'resource_request' in df.columns:\n",
        "            df['requested_cpus'] = df['resource_request'].apply(\n",
        "                lambda x: x.get('cpus', 0) if isinstance(x, dict) else 0\n",
        "            )\n",
        "            df['requested_memory'] = df['resource_request'].apply(\n",
        "                lambda x: x.get('memory', 0) if isinstance(x, dict) else 0\n",
        "            )\n",
        "\n",
        "        if 'scheduling_class' in df.columns:\n",
        "            df['scheduling_class'] = df['scheduling_class'].apply(\n",
        "                lambda x: 'Production' if x >= 120 else 'Non-Production'\n",
        "            )\n",
        "\n",
        "        df['cluster'] = cluster\n",
        "        return df\n",
        "\n",
        "    def preprocess_instance_usage(self, cluster: str, limit: int = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Preprocess resource util.\n",
        "        \"\"\"\n",
        "        df = self.query_table(cluster, 'instance_usage', limit)\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        df = self.convert_time_to_datetime(df, 'start_time')\n",
        "        df = self.convert_time_to_datetime(df, 'end_time')\n",
        "\n",
        "        for usage_col in ['average_usage', 'maximum_usage']:\n",
        "            if usage_col in df.columns:\n",
        "                df[f'{usage_col}_cpus'] = df[usage_col].apply(\n",
        "                    lambda x: x.get('cpus', 0) if isinstance(x, dict) else 0\n",
        "                )\n",
        "                df[f'{usage_col}_memory'] = df[usage_col].apply(\n",
        "                    lambda x: x.get('memory', 0) if isinstance(x, dict) else 0\n",
        "                )\n",
        "\n",
        "        df['cluster'] = cluster\n",
        "        return df\n",
        "\n",
        "    def sample_data(self, table: str, clusters: list = None, limit: int = 10000) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Sample data.\n",
        "        \"\"\"\n",
        "        if clusters is None:\n",
        "            clusters = self.clusters\n",
        "\n",
        "        combined_df = pd.DataFrame()\n",
        "\n",
        "        for cluster in clusters:\n",
        "            logger.info(f\"Sampling {table} from cluster {cluster}\")\n",
        "\n",
        "            if table == 'instance_events':\n",
        "                df = self.preprocess_instance_events(cluster, limit)\n",
        "            elif table == 'instance_usage':\n",
        "                df = self.preprocess_instance_usage(cluster, limit)\n",
        "            else:\n",
        "                df = self.query_table(cluster, table, limit)\n",
        "\n",
        "            if not df.empty:\n",
        "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def export_to_csv(self, df: pd.DataFrame, filename: str) -> None:\n",
        "        \"\"\"\n",
        "        Export to CSV.\n",
        "        \"\"\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        logger.info(f\"Exported {len(df)} rows to {filename}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function.\n",
        "    \"\"\"\n",
        "    preprocessor = GoogleClusterDataPreprocessor()\n",
        "\n",
        "    # Sample instance events\n",
        "    logger.info(\"Sampling instance events data...\")\n",
        "    instance_events_df = preprocessor.sample_data('instance_events', limit=50000)\n",
        "\n",
        "    if not instance_events_df.empty:\n",
        "        instance_events_df = instance_events_df.dropna(subset=['collection_id', 'machine_id'])\n",
        "        preprocessor.export_to_csv(instance_events_df, 'google_cluster_instance_events_sample.csv')\n",
        "\n",
        "    # Sample instance usage data\n",
        "    logger.info(\"Sampling instance usage data...\")\n",
        "    usage_df = preprocessor.sample_data('instance_usage', limit=10000)\n",
        "\n",
        "    if not usage_df.empty:\n",
        "        preprocessor.export_to_csv(usage_df, 'google_cluster_usage_sample.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "vyDGmus9Yw-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate your Google Cloud account.\n",
        "\n",
        "Install the required libraries.\n",
        "\n",
        "Run the Python code"
      ],
      "metadata": {
        "id": "4TE2inboZtEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the processed data as CSV files"
      ],
      "metadata": {
        "id": "BXEop8CpZ0pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"google_cluster_instance_events_sample.csv\")\n",
        "files.download(\"google_cluster_usage_sample.csv\")\n"
      ],
      "metadata": {
        "id": "66ygnj28Z199"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}